<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description"
        content="There may be a fundamental problem with interpretability work that attempts to understand neural networks by decomposing their individual activation spaces in isolation: It seems likely to find features of the activations - features that help explain the statistical structure of activation spaces, rather than features of the model - the features the model’s own computations make use of.">
    <link rel="apple-touch-icon" sizes="180x180" href="/favicons/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">

    <title>Activation space interpretability may be doomed - Bilal Chughtai</title>
    
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
        </script>
    
    <link rel="stylesheet" href="../css/style.css">
</head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NN7RTLBRKY"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-NN7RTLBRKY');
</script>

<body>
    
<header>
    <nav>
        <div class="site-name heading-font">Bilal Chughtai &mdash; <a href="../">home
                page</a>
        </div>
    </nav>
</header>

    <div class="container">
        <div class="column left">
            <div class="sticky-div">
                

<nav id="toc" class="table-of-contents">
    <hr class="toc-separator">
    <ul>
        
        <li class="toc-h1">
            <a href="#section-0">Introduction</a>
        </li>
        
        <li class="toc-h1">
            <a href="#section-1">Examples illustrating the general problem</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-2">1. Activations can contain structure of the data distribution that the models themselves don’t ‘know’ about.</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-3">2. The learned feature dictionary may not match the “model’s feature dictionary”.</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-4">3. Activation space interpretability can fail to find compositional structure.</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-5">4: Function approximation creates artefacts that activation space interpretability may fail to distinguish from features of the model.</a>
        </li>
        
        <li class="toc-h1">
            <a href="#section-6">The general problem</a>
        </li>
        
        <li class="toc-h1">
            <a href="#section-7">What can we do about this?</a>
        </li>
        
    </ul>
    <hr class="toc-separator">
</nav>


            </div>
        </div>
        <div class="column main">
            <div class="collapsed-sidebar">
                
            </div>
            <div class="content">
                
<article>
    <h2 class="post-title">Activation space interpretability may be doomed</h2>
    <div class="post-meta">
        
        <p>Co-authored with: <strong>Lucius Bushnaq</strong></p>
        
        <p>
            <time datetime="2025-01-08">8th January 2025</time>
            &middot; 2.4k words &middot; 12 minute read </p>
                <!-- <p class="post-summary"> <strong>Summary</strong>: There may be a fundamental problem with interpretability work that attempts to understand neural networks by decomposing their individual activation spaces in isolation: It seems likely to find features of the activations - features that help explain the statistical structure of activation spaces, rather than features of the model - the features the model’s own computations make use of.</p> -->
    </div>

    
    <div class="collapsed-sidebar">
        <nav id="collapsed-toc" class="table-of-contents">
            <hr class="toc-separator">
            <ul>
                
                <li class="toc-h1">
                    <a href="#section-0">Introduction</a>
                </li>
                
                <li class="toc-h1">
                    <a href="#section-1">Examples illustrating the general problem</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-2">1. Activations can contain structure of the data distribution that the models themselves don’t ‘know’ about.</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-3">2. The learned feature dictionary may not match the “model’s feature dictionary”.</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-4">3. Activation space interpretability can fail to find compositional structure.</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-5">4: Function approximation creates artefacts that activation space interpretability may fail to distinguish from features of the model.</a>
                </li>
                
                <li class="toc-h1">
                    <a href="#section-6">The general problem</a>
                </li>
                
                <li class="toc-h1">
                    <a href="#section-7">What can we do about this?</a>
                </li>
                
            </ul>
            <hr class="toc-separator">
        </nav>
    </div>
    

    <div class="post-content">
        <p><strong>TL;DR:</strong> There may be a fundamental problem with interpretability work that attempts to understand neural networks by decomposing their individual activation spaces in isolation: It seems likely to find <em>features of the activations</em> - features that help explain the statistical structure of activation spaces, rather than <em>features of the model</em> - the features the model’s own computations make use of.</p>
<p><em>Written at Apollo Research</em></p>
<h1 id="section-0">Introduction</h1>
<p><strong>Claim: Activation space interpretability is likely to give us features of the activations, not features of the model, and this is a problem.</strong></p>
<p>Let’s walk through this claim.</p>
<p><strong>What do we mean by activation space interpretability?</strong> Interpretability work that attempts to understand neural networks by explaining the inputs and outputs of their layers in isolation. In this post, we focus in particular on the problem of <em>decomposing</em> activations, via techniques such as <a href="https://arxiv.org/abs/2309.08600">sparse</a> <a href="https://transformer-circuits.pub/2023/monosemantic-features">autoencoders</a> (SAEs), PCA, or just by looking at individual <a href="https://openai.com/index/language-models-can-explain-neurons-in-language-models/">neurons</a>. This is in contrast to interpretability work that leverages the wider functional structure of the model and incorporates more information about how the model performs computation. Examples of existing techniques using such information include <a href="https://arxiv.org/abs/2406.11944">Transcoders</a>, <a href="https://arxiv.org/abs/2405.12241">end2end-SAEs</a> and <a href="https://arxiv.org/abs/2405.10928">joint activation/gradient PCAs</a>. </p>
<p><strong>What do we mean by “features of the activations”?</strong> Sets of features that help explain or make manifest the statistical structure of the model’s activations at particular layers. One way to try to operationalise this is to ask for decompositions of model activations at each layer that try to <a href="https://www.lesswrong.com/posts/G2oyFQFTE5eGEas6m/interpretability-as-compression-reconsidering-sae">minimise the description length of the activations in bits</a>.</p>
<p><strong>What do we mean by “features of the model”?</strong> The set of features the model itself actually thinks in, the decomposition of activations along which its own computations are structured, features that are significant to what the model is <em>doing</em> and how it is doing it. One way to try to operationalise this is to ask for the decomposition of model activations that makes the causal graph of the whole model as manifestly simple as possible: We make each feature a graph node, and draw edges indicating how upstream nodes are involved in computing downstream nodes. To understand the model, we want the decomposition that results in the most structured graph with the fewest edges, with meaningfully separate modules corresponding to circuits that do different things. </p>
<p>Our claim is pretty abstract and general, so we’ll try to convey the intuition behind it with concrete and specific examples.</p>
<h1 id="section-1">Examples illustrating the general problem</h1>
<p>In the following, we will often use SAEs as a stand-in for any technique that decomposes individual activation spaces into sets of features. But we think the problems these examples are trying to point to apply in some form to basically any technique that tries to decompose individual activation spaces in isolation.</p>
<h2 id="section-2">1. Activations can contain structure of the data distribution that the models themselves don’t ‘know’ about.</h2>
<p>Consider a simple model that takes in a two-dimensional input $(x, y)$ and computes some scalar function of the two, $f(x,y)$. Suppose for all data points in the data distribution, the input data $(x, y)$ falls on a very complicated one-dimensional curve. Also, suppose that the trained model is blind to this fact and treats the two input variables as entirely independent (i.e. none of the model’s computations make use of the relationship between $x$ and $y$). If we were to study the activations of this model, we might notice this curve (or transformed curve) and think it meaningful.</p>
<p>In general, data distributions used for training (and often also interpreting) neural networks contain a very large amount of information about the process that <em>created</em> said dataset. For all non-toy data distributions, the distribution will reflect complex statistical relationships of the universe. A model with finite capacity can't possibly learn to make use of all of these relationships. Since activations are just mathematical transformations of inputs sampled from this data distribution, by studying neural networks through their distribution of activations, we should expect to see many of those unused relationships in the activations. So, fully understanding the model’s activations can in a sense be substantially <em>harder</em> than fully understanding what the model is doing. And if we don’t look at the computations the model is carrying out on those activations <em>before</em> we try to decompose them, we might struggle to tease apart properties of the input distribution and properties of the model.</p>
<h2 id="section-3">2. The learned feature dictionary may not match the “model’s feature dictionary”.</h2>
<p>Now let’s consider another one-dimensional curve, this time embedded in a ten-dimensional space. One of the nice things about sparse dictionary methods like SAEs is that they can approximate curves like this pretty well, using a large dictionary of features with sparse activation coefficients. If we train an SAE with a dictionary of size 500 on this manifold, we might find 500 features, only a very small number of which are active at a time, corresponding to different tiny segments of the curve.</p>
<p>Suppose, however, that the model <em>actually</em> thinks of this single dense data-feature as a sparse set of $100$ linear directions. We term this set of directions the “<em>model’s dictionary”.</em> The model’s dictionary approximates most segments of the curve with lower resolution than our dictionary, but it might approximate some crucial segments a lot more finely. MLPs and attention heads downstream in the model carry out computations on these 100 sparsely activating directions. The model’s decomposition of the ten-dimensional space into 100 sparse features and our decomposition of the space into 500 sparse features are necessarily quite different. Some features and activation coefficients in the two dictionaries <em>might</em> be closely related, but we should not expect most to be. If we are not looking at what the model <em>does</em> with these activations downstream, how can we tell that the feature dictionary we find matches the model’s feature dictionary? When we perform the decomposition, we don’t know yet what parts of the curve are more important for what the model is computing downstream, and thus how the model is going to think about and decompose the ten-dimensional subspace. We probably won’t even be aware in the first place that the activations we are decomposing lie on a one-dimensional curve without significant <a href="https://arxiv.org/abs/2405.14860">extra work</a>. </p>
<h2 id="section-4">3. Activation space interpretability can fail to find compositional structure.</h2>
<p>Suppose our model represents four types of object in some activation space: {blue square, red square, blue circle, red circle}. We can think of this as the direct product space {blue, red} $\otimes$ {square, circle}. Suppose the model’s 'true features' are colour and shape, in the sense that later layers of the model read the 'colour' variable and the 'shape' variable independently.  Now, suppose we train an SAE with 4 dictionary elements on this space. SAEs are optimised to achieve high sparsity -- few latents should be active on each forward pass. An SAE trained on this space will therefore learn the four latents {blue square, red square, blue circle, red circle} (the "composed representation"), rather than {blue, red} $\otimes$ {square, circle} (the "product representation"), as the former has sparsity 1, while the latter has sparsity 2. In other words, the SAE learns <a href="https://www.lesswrong.com/posts/QoR8noAB3Mp2KBA4B/do-sparse-autoencoders-find-true-features">features</a> <a href="https://www.lesswrong.com/posts/a5wwqza2cY3W7L9cj/sparse-autoencoders-find-composed-features-in-small-toy">that</a> <a href="https://www.lesswrong.com/posts/tojtPCCRpKLSHBdpn/the-strong-feature-hypothesis-could-be-wrong">are</a> <a href="https://arxiv.org/abs/2407.14662">compositions</a> of the model’s features. </p>
<p>Can we fix this by adjusting the sparsity penalty? Probably not. Any sparse-dictionary approach set to decompose the space as a whole will likely learn this same set of four latents, as this latent set is sparser, with <a href="https://arxiv.org/abs/2410.11179">shorter description length</a> than the product set. </p>
<p>While we <em>could</em> create some ansatz for our dictionary learning approach that specifically privileges the product configuration, this is cheating. How would we know the product configuration and not the composed configuration matches the structure of the model’s downstream computations in this case in advance, if we only look at the activations in isolation? And even if we do somehow know the product configuration is right, how would we know in advance to look for this specific 2x2 structure? In reality, it would additionally be embedded in a larger activation space with an unknown number of further latents flying around besides just shape and colour. </p>
<h2 id="section-5">4: Function approximation creates artefacts that activation space interpretability may fail to distinguish from features of the model.</h2>
<p>This one is a little more technical, so we’ll take it in two stages. First, a very simplified version, then something that’s closer to the real deal.</p>
<p><strong>Example 4a: Approximating</strong> $x^2$. Suppose we have an MLP layer that takes a scalar input $x$ and is trained to approximate the scalar output $x^2$. The MLP comprises a $\text{W}_{in}$ matrix (vector, really) of shape $(1, 10)$ that maps $x$ to some pre-activation. This gets mapped through a ReLU, giving a $10$ dimensional activation vector $a$. Finally, these are mapped to a scalar output via some $\text{W}_{out}$ matrix of shape $(10, 1)$. Thus, concretely, this model is tasked with approximating $x^2$ via a linear combination of ten functions of the form $\text{ReLU}(ax+b)$. Importantly, the network only cares about one direction in the 10 dimensional activation space, the one which gives a good approximation of $x^2$ and is projected off by $\text{W}_{out}$. There are 9 other orthogonal directions in the hidden space. Unless we know in advance that the network is trying to compute $x^2$, this important direction will not stick out to us. If we train an SAE on the hidden activations, or do a PCA, or perform any other activation decomposition of our choice, we will get out a bunch of directions, and likely none of them will be $x^2$. What makes the $x^2$ direction special is that the model uses it downstream (which, here, means that this direction is special in $\text{W}_{out}$). But that information can't be found in the hidden activations alone. We need more information. </p>
<p><strong>Example 4b: Circuits in superposition</strong>. The obvious objection to Example 4a is that $\text{W}_{out}$ is natively a rank one matrix, so the fact that only one direction in the 10 dimensional activation space matters is trivial and obvious to the researcher. So while we do need to use some information that isn’t in the activations, it’s a pretty straightforward thing to find. But if we extend the above example to something more realistic, it’s not so easy anymore. Suppose the model is computing a bunch of the above multi-dimensional <a href="https://www.lesswrong.com/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural">circuits in superposition</a>. For example, take an MLP layer instead with 40,000 neurons, computing 80,000 functions of (sparse) scalar inputs, each of which requires 10 neurons to compute, and writes the results to a 10,000 dimensional residual stream. Each of these 80,000 circuits would then occupy some ten-dimensional subspace in the 40,000 dimensional activation space of the MLP, meaning the subspaces must overlap. Each of these subspaces may only have one direction that actually matters for downstream computation. </p>
<p>Our SAE/PCA/activation-decomposition-of-choice trained on the activation space will not be able to tell which directions are actually used by the model, and which are an artefact of computing the directions that do matter. They will decompose these ten-dimensional subspaces into a bunch of directions, which almost surely won’t line up with the important ones. To make matters worse, we might not immediately know that something went wrong with our decomposition. All of these directions might look like they relate to some particular subtask when studied through the lens of e.g. max activating dataset examples, since they’ll cluster along the circuit subspaces to some extent. So the decomposition could actually look very interesting and interpretable, with a lot of directions that appear to somewhat-but-not-quite make sense when we study them. However, these many directions will seem to interact with the next layer in a very complicated manner.</p>
<h1 id="section-6">The general problem</h1>
<p>Not all the structure of the activation spaces matters for the model’s computations, and not all the structure of the model’s computations is manifest in the structure of individual activation spaces. </p>
<p>So, if we are trying to understand the model by <em>first</em> decomposing its activation spaces into features and <em>then</em> looking at how these features interact and form circuits, we might get a complete mess of interactions that do not make the structure of the model and what it is doing manifest at all. We need to have at least some relevant information about how the model itself uses the activations <em>before</em> we pick our features, and include that information in the activation space decomposition. Even if our goal is just to understand an aspect of the model’s representation enough for a use case like monitoring, looking at the structure of the activation spaces rather than the structure of the model’s computations can give us features that don’t have a clean causal relationship to the model’s structure and which thus might mislead us. </p>
<h1 id="section-7">What can we do about this?</h1>
<p>If the problem is that our decomposition methodologies lack relevant information about the network, then maybe the solution is giving them more of it. How could we try to do this?</p>
<p><strong>Guess the correct</strong> <a href="https://en.wikipedia.org/wiki/Ansatz"><strong>ansatz</strong></a><strong>.</strong> We can try to make a better ansatz for our decompositions by guessing in advance how model computations are structured. This requires progress on interpretability fundamentals, through e.g. understanding the structure and purpose of <a href="https://www.lesswrong.com/posts/MFBTjb2qf3ziWmzz6/sae-feature-geometry-is-outside-the-superposition-hypothesis">feature geometry</a> better. Note however that the current favoured roadmap for making progress on those topics seems to be “decompose the activations well, understand the resulting circuits and structure, and then hope this yields increased understanding”. This may be a bit of a chicken-and-egg situation. </p>
<p><strong>Use activations (or gradients) from more layers.</strong> We can try to use information from more layers to look for decompositions that simplify the model as a whole. For example, we can decompose multiple layers simultaneously and impose a sparsity penalty on connections between features in different layers. Other approaches that fall vaguely in this category include <a href="https://arxiv.org/pdf/2405.12241">end-to-end-SAEs,</a> <a href="https://transformer-circuits.pub/2024/april-update/index.html#attr-dl">Attribution Dictionary Learning,</a> <a href="https://arxiv.org/abs/2406.11944">Transcoders</a>, and <a href="https://transformer-circuits.pub/2024/crosscoders/index.html">Crosscoders</a>.</p>
<p><strong>Use weights instead of or to supplement activations.</strong> Most interpretability work studies activations and not weights. There are good reasons for this: activations are lower dimensional than weights. The curse of dimensionality is real. However, weights, in a sense, contain the entire functional structure of the model, because they <em>are</em> the model. It seems in principle possible to decompose weights into circuits <em>directly</em>, by minimising some complexity measure over some kind of weight partitioning, without any intermediary step of decomposing activations into features at all. This would be a reversal of the standard, activations-first approach, which aims to understand features first and later understand the circuits. Apollo Research are currently trying this.</p>
<p><em>Thanks to Andy Arditi, Dan Braun, Stefan Heimersheim and Lee Sharkey for feedback.</em></p>
    </div>
    
    <div class="footer">
        
        
        
        <p><em>There may be more information or discussion related to this post elsewhere:</em></p>
        <ul>
            
            <li><a href="https://www.lesswrong.com/posts/gYfpPbww3wQRaxAFD/activation-space-interpretability-may-be-doomed">www.lesswrong.com</a></li>
            
        </ul>
        </p>
        
    </div>
</article>

            </div>
        </div>
        <!--<div class="column right">
            <div class="sticky-div">
                
            </div>
        </div>-->
    </div>
    <footer></footer>

    <!-- Footnote tooltip script -->
    <script>
        document.addEventListener('DOMContentLoaded', function () {
            // Create a single tooltip element
            const tooltip = document.createElement('div');
            tooltip.className = 'footnote-tooltip';
            document.body.appendChild(tooltip);
            let hideTimer;

            function positionTooltip(ref) {
                const rect = ref.getBoundingClientRect();
                const scrollX = window.scrollX || window.pageXOffset;
                const scrollY = window.scrollY || window.pageYOffset;
                // Initial placement below the reference
                let top = scrollY + rect.bottom + 8;
                let left = scrollX + rect.left;
                tooltip.style.top = top + 'px';
                tooltip.style.left = left + 'px';
                // Adjust if it overflows the right edge
                const tt = tooltip.getBoundingClientRect();
                const viewportWidth = document.documentElement.clientWidth;
                if (tt.right > viewportWidth - 8) {
                    const overflow = tt.right - (viewportWidth - 8);
                    tooltip.style.left = (left - overflow) + 'px';
                }
                // Adjust if it overflows the left edge
                if (tt.left < 8) {
                    tooltip.style.left = (scrollX + 8) + 'px';
                }
            }

            function showTooltip(ref) {
                clearTimeout(hideTimer);
                tooltip.innerHTML = ref.getAttribute('data-footnote');
                tooltip.style.display = 'block';
                positionTooltip(ref);
            }

            function hideTooltip() {
                hideTimer = setTimeout(() => {
                    tooltip.style.display = 'none';
                }, 300); // 300ms delay before hiding
            }

            // Attach events to each footnote reference
            document.querySelectorAll('a.footnote-ref').forEach(function (ref) {
                ref.addEventListener('mouseenter', () => showTooltip(ref));
                ref.addEventListener('mouseleave', hideTooltip);
            });

            // Keep tooltip open when hovering over it
            tooltip.addEventListener('mouseenter', () => clearTimeout(hideTimer));
            tooltip.addEventListener('mouseleave', hideTooltip);
        });
    </script>
</body>

</html>