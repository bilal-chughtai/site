<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="I am about to start working on a frontier lab safety team. This post presents a varied set of perspectives that I collected and thought through before accepting my offer.">
    <link rel="icon" type="image/svg+xml" href="../assets/favicon.svg">
    <title>Reasons for and against working on technical AI safety at a frontier AI lab - Bilal Chughtai</title>
    
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    
<header>
    <nav>
        <div class="site-name heading-font">Bilal Chughtai &mdash; <a href="../">home page</a>
        </div>
    </nav>
</header>

    <div class="container">
        <div class="column left">
            <div class="sticky-div">
                

<nav id="toc" class="table-of-contents">
    <hr class="toc-separator">
    <ul>
        
        <li class="toc-h1">
            <a href="#section-0">For</a>
        </li>
        
        <li class="toc-h1">
            <a href="#section-1">Against</a>
        </li>
        
        <li class="toc-h1">
            <a href="#section-2">Disclaimers</a>
        </li>
        
    </ul>
    <hr class="toc-separator">
</nav>


            </div>
        </div>
        <div class="column main">
            <div class="collapsed-sidebar">
                
            </div>
            <div class="content">
                
<article>
    <h2 class="post-title">Reasons for and against working on technical AI safety at a frontier AI lab</h2>
    <div class="post-meta">
        
        <p>
            <time datetime="2024-12-26">2024-12-26</time>
            &middot; 3.6k words 
            &middot; 13 minute read
        </p>
        <!-- <p class="post-summary"> <strong>Summary</strong>: I am about to start working on a frontier lab safety team. This post presents a varied set of perspectives that I collected and thought through before accepting my offer.</p> -->
    </div>

    
    <div class="collapsed-sidebar">
        <nav id="collapsed-toc" class="table-of-contents">
            <hr class="toc-separator">
            <ul>
                
                <li class="toc-h1">
                    <a href="#section-0">For</a>
                </li>
                
                <li class="toc-h1">
                    <a href="#section-1">Against</a>
                </li>
                
                <li class="toc-h1">
                    <a href="#section-2">Disclaimers</a>
                </li>
                
            </ul>
            <hr class="toc-separator">
        </nav>
    </div>
    

    <div class="post-content">
        <p><em>I am about to start working on a frontier lab safety team. This post presents a varied set of perspectives that I collected and thought through before accepting my offer. Thanks to the many people I spoke to about this.</em> </p>
<h1 id="section-0">For</h1>
<p><strong>You're close to the action</strong>. As AI continues to <a href="https://www.lesswrong.com/posts/jb4bBdeEEeypNkqzj/orienting-to-3-year-agi-timelines">heat</a> <a href="https://www.lesswrong.com/posts/7jn5aDadcMH6sFeJe/why-i-m-joining-anthropic">up</a>, being closer to the action seems increasingly important. Being at a frontier lab allows you to better understand how frontier AI development actually happens and make better predictions about how it might play out in future. You can build a <a href="https://www.lesswrong.com/posts/nEBbw2Bc2CnN2RMxy/gears-level-models-are-capital-investments">gears level model</a> of what goes into the design and deployment of current and future frontier systems, and the bureaucratic and political processes behind this, which might inform the kinds of work you decide to do in future (and more broadly, your life choices). </p>
<p><strong>Access to frontier models, compute, and infrastructure.</strong> Many kinds of prosaic safety research benefit massively from having direct and elevated access to frontier models and infrastructure to work with them. For instance: <a href="https://anthropic.com/responsible-scaling-policy">Responsible Scaling Policy</a> focussed work that directly evaluates model capabilities and mitigations against specific threat models, <a href="https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1">model organisms</a> work that builds demonstrations of threat models to serve as a testing ground for safety techniques and <a href="https://arxiv.org/abs/2211.03540">scalable oversight</a> work attempting to figure out how to bootstrap and amplify our ability to provide oversight to models in the superhuman regime, to name a few. Other safety agendas might also benefit from access to large amounts of compute and infrastructure: e.g. mechanistic interpretability currently seems to be moving in a <a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">more compute-centric direction</a>. Labs are very well <em>resourced</em> in general, and have a large amount of funding that can be somewhat flexibly spent as and when needed (e.g. on contractors, data labellers, etc). Access to non-public models potentially significantly beyond the public state of the art might also generically speed up all work that you do.</p>
<p><strong>Much of the work frontier labs do on empirical technical AI safety is the best in the world.</strong> AI safety is talent constrained. There are still not enough people pushing on many of the directions labs work on. By joining, you increase the labs capacity to do such work. If this work is published, this may have a positive impact on safety at all frontier labs. If not, you may still directly contribute to future AGIs built by your lab being safer, either through informing deployment decisions or through research that eventually makes its way into frontier models. The metric of success for lab safety work seems closer to "actually improve safety" than e.g. "publish conference papers".</p>
<p><strong>Often shorter route to impact</strong>. Technical safety work can only have an impact if it either directly or indirectly influences some future important deployed system. The further you are from such a system, the lower your influence might be. For the kinds of work that strive to <em>directly</em> improve safety, if you aren't at the important lab itself, the causal impact chain must route through people who directly touch the system(s) of importance reading your work, thinking it is good enough to change what they are doing, and then using your ideas. Relatedly, if AGI timelines are short, there is less time for external or earlier stage work to percolate into lab thinking. If you are at the lab, the causal chain becomes much shorter; it is someone in your management line's job to convince relevant stakeholders that your work is important for improving the safety of the future important deployed system (though note you might not always be able to rely on this mechanism working effectively). That said, plenty of external technical work can also have a large impact. This is often (but not always) through work whose goal is to <em>indirectly</em> influence future systems. I discuss this point in more detail later.</p>
<p><strong>Intellectual environment</strong>. Frontier labs generally have a very high saturation of smart, ambitious, talented and experienced people. Having competent collaborators accelerates your work. Mentorship accelerates your development as a technical contributor. More broadly, your intellectual environment really matters, and can make a big difference on both your happiness and outputs. Where you work directly influences who you talk to on a day to day basis, which feeds into feedback on your work, which feeds into your work quality, which feeds into your eventual impact. Labs are not the only place with high densities of people thinking carefully about how to make AI go well, but are one of a select few such places.</p>
<p><strong>Career capital</strong>. Working at a frontier lab continues to offer a large amount of career capital. It is among the best ways to gain prosaic AI-specific research and engineering skills. It is arguably even more prestigious and high status now than it used to be, as (general) AI rapidly becomes more and more important in the world. Frontier labs compensate their technical staff extremely well. Besides the obvious benefits, money increases your runway and ability to pursue riskier paths later in life, and capacity to fund progress on top world problems (see <a href="https://www.givingwhatwecan.org/best-charities-to-donate-to-2024">GWWC</a> or <a href="https://forum.effectivealtruism.org/posts/s9dyyge6uLG5ScwEp/it-looks-like-there-are-some-good-funding-opportunities-in">this advice</a> for giving opportunities in AI safety). If you believe that AGI is only a few years away and will make human intellectual labour obsolete, accruing wealth in advance of that point seems potentially even <a href="https://benjamintodd.substack.com/p/how-can-an-ordinary-person-prepare">more</a> <a href="https://nosetgauge.substack.com/p/capital-agi-and-human-ambition">important</a> than normal throughout history. The prospects of ex-lab employees are generally strong, and their opinions are respected by a wide range of people. For instance, an OpenAI whistleblower recently testified in front of a <a href="https://www.judiciary.senate.gov/committee-activity/hearings/oversight-of-ai-insiders-perspectives">senate committee</a> on matters of AI safety, and ex-lab employees (much like ex-FAANG employees) generally have an easy time raising VC funding for startup ventures. On the flip side, there are several <strong>career risks</strong> to working at a frontier lab worth considering. It seems possible (likely?) that there will be some non-existential AI powered catastrophe in the next few years, and that this may worsen the reputation of AI labs and thus change the prospects of AI lab researchers. Another risk is that working at an AI lab may "tarnish" your reputation and ability to later work in government or strategy positions (though empirically, many ex-lab employees still end up doing this, and working at a lab also increases your ability to work in such a position in other ways). </p>
<p><strong>Making the lab you work for more powerful might be good, actually.</strong> Indirect impact may come via it actually being <a href="https://80000hours.org/career-reviews/working-at-an-ai-lab/#force-for-good-or-bad">good for the lab you work for to be more powerful</a>. For example, you might believe that your lab will act sufficiently safely and responsibly with their eventual AGI, shift industry culture to be more pro-safety, do valuable safety work with their powerful models, or advocate for good regulation. This argument necessarily varies considerably across labs, and can’t be true for all labs at once – so be careful applying this argument.</p>
<h1 id="section-1">Against</h1>
<p><strong>Some very important safety work happens outside of frontier AI labs.</strong> For instance, external organizations such as <a href="https://www.nist.gov/aisi">AI Safety</a> <a href="https://www.aisi.gov.uk/">Institutes</a>, <a href="https://www.apolloresearch.ai/">Apollo Research</a> and <a href="https://metr.org/">METR</a> conduct dangerous capability evaluations of frontier models. On top of directly evaluating risk, they shape the public discussion of AI risk significantly, and may have more hard power in future. While this work does happen at frontier labs too, there are both good reasons for it to happen externally, and external organizations provide further manpower on the direction over what might be capable at the labs alone. External organizations are also able to legibly challenge the positions AI labs hold, by for example, suggesting that historic deployment decisions were actually dangerous. Work directly challenging the positions held by AI labs may become more important over time as lab profit incentives to deploy unsafe systems increase. More broadly, the types of research that happen at labs are generally those that are comparatively advantaged to happen at labs (i.e. those that require access to frontier models, compute, and infrastructure – see above). This means there are plenty of types of technical AI safety work that <em>don’t</em> happen at labs and which might be important. The most salient examples are highly theoretical work, such as what <a href="https://www.alignment.org/">ARC</a> currently does or the agent foundations work <a href="https://intelligence.org/">MIRI</a> used to do. John Wentworth argues a more cynical take <a href="https://www.lesswrong.com/posts/nwpyhyagpPYDn4dAW/the-field-of-ai-alignment-a-postmortem-and-what-to-do-about">here</a> that lab work is uniformly streetlighty, and doesn’t tackle the hard safety problems. See also the <a href="https://jobs.80000hours.org/?refinementList%5Btags_area%5D%5B0%5D=AI%20safety%20%26%20policy">80000 hours job board</a> for further roles outside of frontier labs.</p>
<p><strong>Low neglectedness</strong>. While it might well be the case that the work happening at a frontier lab is both important and tractable, it's possible it's not all that neglected. Many more people want to work on frontier lab safety teams than there is capacity to hire. This oversupply should not be at all surprising; as discussed above, working at a lab is a high paying, stable-ish and prestigious career path.  Supposing you do get an offer, it’s pretty unclear how <a href="https://80000hours.org/2019/08/how-replaceable-are-top-candidates-in-large-hiring-rounds/">replaceable</a> you are: the next best hire may (or may not) be all that much worse than you. It currently feels like <a href="https://www.lesswrong.com/posts/Z87fSrxQb4yLXKcTk/mats-winter-2023-24-retrospective#:~:text=23%20scholars%20submitted%20a%20survey%20indicating%20which%20organizations%20they%20would%20most%20like%20to%20see%20at%20the%20event">everyone and their dog wants to work at a frontier lab</a> (and this effect is likely larger outside of our bubble), and that an entire generation of smart, agentic and motivated individuals who care a lot about making AI go well are ending up at the frontier labs. Is this really optimal? On the one hand, it seems a shame that incentive gradients suck everyone into working at the same places, on the same problems, and converging on similar views. See <a href="https://x.com/TylerAlterman/status/1861835489579307208">here</a> and <a href="https://www.lesswrong.com/posts/nwpyhyagpPYDn4dAW/the-field-of-ai-alignment-a-postmortem-and-what-to-do-about">here</a> for more extreme versions of this take. On the other hand, I would much rather have AI labs staffed by such people than by status-climbing individuals who care less about the mission.</p>
<p><strong>Low intellectual freedom.</strong> Wherever you work, unless you are really quite senior or otherwise given an unusually large amount of freedom over what you work on, you should expect the bulk of your impact to come through accelerating some existing agenda. In <a href="https://www.lesswrong.com/posts/7LZHS4afrXCNuuGK9/book-summary-zero-to-one">Peter Thiel’s</a> language, this is like going from “one to n”. To the extent you believe that such an agenda is good and useful, this is great! But is it the best possible use of your time? Are there places you think people are obviously <a href="https://www.lesswrong.com/posts/Zp6wG5eQFLGWwcG6j/focus-on-the-places-where-you-feel-shocked-everyone-s">dropping the ball</a>? If you are comparatively advantaged to work on something that seems comparably important but significantly more neglected, and have a track record (or just sufficient drive) for succeeding in doing your own thing, it may be of higher expected value to consider doing that instead. Even if you don’t have any such ideas, it might be worth considering asking others for advice, taking time to explore, brainstorming, and iterating anyway. Most existing promising AI safety agendas were not born at frontier labs. They were cultivated elsewhere, and eventually <em>imported</em> to labs, once sufficient promise was shown (the most recent such example is <a href="https://arxiv.org/abs/2312.06942">AI control</a>, which was pioneered by <a href="https://www.redwoodresearch.org/">Redwood Research</a>). There are several good essays online that discuss how ambitious individuals should orient to maximize their chances of doing <a href="https://paulgraham.com/greatwork.html">great</a> <a href="https://www.lesswrong.com/posts/R5yL6oZxqJfmqnuje/cultivating-a-state-of-mind-where-new-ideas-are-born">work</a>; they all emphasise the importance of freedom to work on your own questions, ideas and projects. AI safety might need more novel bets that take us from “zero to one”. Most people will struggle to execute their own highly exploratory and highly risky research bets at labs. Various other places seem better suited for such work; for instance, a PhD offers a large amount of freedom and seems like a uniquely good place to foster the skill of developing <a href="https://www.lesswrong.com/posts/PdtkXcgbRpdHWRNt6/you-should-consider-applying-to-phds-soon">research taste</a>, though has other downsides. Some counterarguments to this are that timelines might be short, so you may not have a good idea externally in time for it to matter, and that there are strong personal incentives against this (e.g. see the above career capital section). Finally, “making AI go well” requires so much more than just technical safety work, and may indeed be bottlenecked on some of <a href="https://www.governance.ai/">these</a> <a href="https://www.lesswrong.com/posts/KFFaKu27FNugCHFmh/by-default-capital-will-matter-more-than-ever-after-agi">other</a> <a href="https://www.lesswrong.com/posts/Mak2kZuTq8Hpnqyzb/the-intelligence-curse">problems</a>, which some (but by no means all) would-be lab researchers seem particularly well placed to carry out. Beyond “ability to do technical AI safety research”, technical AI safety researchers have a number of skills and unique beliefs about the world that might prove useful in pursuing such other routes to impact, via for instance entrepreneurship or policy.</p>
<p><strong>Shifting perspectives</strong>. Working at a frontier lab will likely change your views about AI safety in ways that your present self may not endorse. This may happen slowly and sneakily, in a way that you might not locally notice.  You should acknowledge and accept that your perspectives may change before you join. I think of this as mostly a negative, but it’s also possible that your views move closer to the truth, if people at the lab hold a more correct view than you do. The exact mechanisms behind how this happens are not clear to me, but may include some of the following causes.</p>
<ul>
<li><strong>Information environment.</strong> Your information environment has a large influence on your views. Your information environment includes what you read and who you talk to every day. To first approximation, you should expect your views to move towards the median of your information environment, unless you are very sure of your views and extremely good at arguing for them. Lab perspectives are likely different to those of the wider AI safety community, the ML community, and the wider world. The median person at a frontier lab may be less scared about future systems than you might be, and more optimistic that we are on track to succeed in building AGI safely. That said, you might not be surrounded by the median lab person, especially if the lab is very large and has a very diffuse culture. Relatedly, there may also be some risk overly <a href="https://forum.effectivealtruism.org/posts/LKdhv9a478o9ngbcY/deferring">deferring</a> to the views of your seniors.</li>
<li><strong>Financial incentives.</strong> You are extremely <a href="https://www.lesswrong.com/posts/Be3ertyJfwDdQucdd/how-should-turntrout-handle-his-deepmind-equity-situation">financially correlated</a> with the success of the lab, which might incentivize making risky strategic decisions. This might make it harder to think objectively about risks. I would be especially worried about this if I were a key decision maker behind deployment decisions, and less the further removed I am from such a position. I don’t think being extremely far removed from decision making reduces this risk to zero though. One concern might be that financial incentives shape your worldview, such that future decisions you make (in perhaps a more senior capacity) may differ. For labs where your equity can be publicly traded (e.g. GDM or Meta), this is somewhat less of an issue than at labs where you can only rarely sell your stock options (e.g. Anthropic and OpenAI). If you decide that remaining at the lab is a bad idea and want to leave, you may still have various conflicts of interest (e.g. unsold equity) and constraints in what you can discuss publicly (e.g. via NDAs) even after leaving. Notably, prior to May 2024, OpenAI used financial incentives to get employees to sign non-disparagement agreements upon leaving. Note further that the vesting schedules for equity may incentivise you to stay at a frontier lab longer than you might like if you do decide you want to leave.</li>
</ul>
<p><strong>It might be hard to influence the lab.</strong> A common belief is that by joining a frontier lab and advocating for safety, you might be able to change the lab's perspectives and prioritisation. While there is some truth to this, this is probably far harder than you think. For instance, in spring 2024, many safety focussed employees (some of which were <em>extremely</em> senior) left OpenAI, after having lost confidence in OpenAI leadership to sufficiently prioritise safety, despite their internal pressures. It may be possible to shift your team’s local perspectives on safety, but you should expect it to be substantially harder to change the views of the organisation as a whole. On the flip side, employees certainly have some power – employee support is why Sam Altman remains the CEO of OpenAI today after the <a href="https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI">board fiasco of 2023</a>. Relatedly, the lab environment may influence the kinds of work you do in ways you don’t expect: there may be incentives to produce work that supports lab leadership’s desired “vibe”; their vision for what they want to achieve and communicate – rather than maximally scientifically helpful or impactful work.</p>
<p><strong>Safetywashing</strong>. Your work may be used for <a href="https://arxiv.org/abs/2407.21792">safetywashing</a>; it may be exploited for PR while either doing nothing to improve safety or even differentially improving capabilities rather than safety. This of course depends quite heavily on what your exact role is. Note too that just because you currently think your work might not have this negative externality, this does not mean it won't in future. You might be moved to working on projects which are less good on this axis. It might be hard for you to realise this is happening at the time, even harder for you to do something about it, and impossible to predict ahead of time. It might be a good idea to <a href="https://www.benkuhn.net/abyss/">stare in to the abyss</a> often and ask yourself if your work remains good for the world, though it might be stressful having to constantly make this sort of evaluation. How much you should weigh the safetywashing concern might also depend on the degree of trust you put in your labs leadership to make responsible decisions. </p>
<p><strong>Speaking publicly.</strong> You might be restricted or otherwise constrained in what you can talk about publicly, especially on topics relating to AI timelines or AI safety. The extent to which this is the case seems to differ wildly across labs. On top of explicit restrictions, you might also be implicitly disincentivized from speaking about or doing things that your colleagues or seniors may disapprove of. For instance, you may think that <a href="https://pauseai.info/">PauseAI</a> are doing good work, but struggle to publicly support it. If AGI projects become nationalized and lab security increases substantially, there may be greater restrictions on your personal life.</p>
<p><strong>External collaborations.</strong> The degree to which you can collaborate with the wider safety community on projects and research might be restricted. This again often depends on role specific details. For instance, the Anthropic interpretability team generally do not talk about non-public research and also generally do not collaborate externally. In contrast, the <a href="https://alignment.anthropic.com/2024/anthropic-fellows-program/">Anthropic alignment science</a> and <a href="https://www.matsprogram.org/interpretability">GDM interpretability</a> teams engage and collaborate more widely. Uniformly, you should expect your ability to engage in external policy and strategy related projects to be heavily restricted. Though if you are early career, your legibility increases by being at a lab, somewhat counteracting this point.</p>
<p><strong>Bureaucracy</strong>. Labs often have a bunch of irritating bureaucracy that makes various things harder. Publishing papers and open sourcing code or models is challenging. There are often pressures or constraints incentivizing employees to use in-house infra, even if it is worse than open source tooling. Internal non-meritocratic politics can sometimes play a role in what work teams are allowed to do: there often exists internal competition between teams over access to resources and ability to ship. Finally, lab legal and comms teams are set up to prevent bad things happening, rather than to make good things happen, which can sometimes slow things down. Downside risk is much more important for large actors than potential upside. Note that many of these points are not unique to frontier labs, but to large organizations in general. The flip-side of this is that bureaucracy also often protects technical contributors from worrying about various forms of legal and financial risk that smaller actors have to worry about more. Being part of a large and stable organization also often ensures various basics are taken care of; individual contributors don’t need to worry about things such as office space, food, IT, etc. </p>
<p><strong>AGI seems inherently risky.</strong> AGI will dramatically alter humanity's trajectory if achieved, for better or for worse. One possible future is one in which AGI causes a large amount of harm and threatens humanities extinction. Each lab working on creating AGI may be shortening timelines and bringing us closer to such a future. The effects of AGI on the world are complex to model and predict, but it feels reasonable to feel bad about working at an organisation building such a technology given uncertainty and plausible downside risk on non-consequentialist deontological grounds, even if your role promotes safety.</p>
<h1 id="section-2">Disclaimers</h1>
<ul>
<li><a href="https://80000hours.org/career-reviews/working-at-an-ai-lab/">80000 hours</a> have previously discussed some of these considerations. In this piece I discuss many additional considerations. See also their more <a href="https://80000hours.org/articles/harmful-career/">general considerations</a> of whether it is ever okay to work at a harmful company to do good.</li>
<li>This post is targeted at people considering working on technical AI safety at a frontier lab. Some considerations will generalise to people considering other roles at frontier labs, or those considering working on technical AI safety at other organisations.</li>
<li>In an attempt to make this post maximally useful to a wide audience, I do not compare to specific counterfactual options, but encourage readers considering such a role to think through these when reading.</li>
<li>Many of these points have high variance both across labs, and across teams and roles within the same lab.</li>
<li>Many of these points are subtle, and not strict pros or cons. I try to convey such nuance in the writing under each point, and list the points under the heading that most makes sense to me.</li>
<li>Despite using the term “lab” throughout, AI labs are now best thought of as “companies”. They no longer just do research, and profit incentives increasingly play a role in lab strategy.</li>
</ul>
    </div>
    <div class="footer">
        
        
        
        <p>There may be comments and footnotes on the version of this post that was published on:</p>
        <ul>
            
            <li><a href="https://www.lesswrong.com/posts/cyYgdYJagkG4HGZBk/reasons-for-and-against-working-on-technical-ai-safety-at-a">www.lesswrong.com</a></li>
            
            <li><a href="https://forum.effectivealtruism.org/posts/vCZxmMP2dDjxFByrD/reasons-for-and-against-working-on-technical-ai-safety-at-a">forum.effectivealtruism.org</a></li>
            
        </ul>
        </p>
        
    </div>
</article>

            </div>
        </div>
        <!--<div class="column right">
            <div class="sticky-div">
                
            </div>
        </div>-->
    </div>
    <footer></footer>
</body>
</html>