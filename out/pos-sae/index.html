<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <link rel="icon" type="image/svg+xml" href="../assets/favicon.svg">
    <title>Understanding positional features in layer 0 SAEs - Bilal Chughtai</title>
    
    <script>
    MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']],
        displayMath: [['$$', '$$'], ['\\[', '\\]']],
        processEscapes: true,
        processEnvironments: true
      },
      options: {
        skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    };
    </script>
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    
<header>
    <nav>
        <div class="site-name heading-font">Bilal Chughtai &mdash; <a href="../">home page</a>
        </div>
    </nav>
</header>

    <div class="container">
        <div class="column left">
            <div class="sticky-div">
                

<nav id="toc" class="table-of-contents">
    <hr class="toc-separator">
    <ul>
        
        <li class="toc-h2">
            <a href="#section-0">Summary</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-1">Set Up</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-2">Finding features</a>
        </li>
        
        <li class="toc-h3">
            <a href="#section-3">Positional features</a>
        </li>
        
        <li class="toc-h3">
            <a href="#section-4">Semantic features</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-5">Length generalization of SAEs</a>
        </li>
        
        <li class="toc-h3">
            <a href="#section-6">Positional features break</a>
        </li>
        
        <li class="toc-h3">
            <a href="#section-7">Semantic features break</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-8">Discussion</a>
        </li>
        
        <li class="toc-h2">
            <a href="#section-9">Acknowledgements</a>
        </li>
        
    </ul>
    <hr class="toc-separator">
</nav>


            </div>
        </div>
        <div class="column main">
            <div class="collapsed-sidebar">
                
            </div>
            <div class="content">
                
<article>
    <h2 class="post-title">Understanding positional features in layer 0 SAEs</h2>
    <div class="post-meta">
        
        <p>Co-authored with: <strong>Yeu-Tong Lau</strong></p>
        
        <p>
            <time datetime="2024-07-29">2024-07-29</time>
            &middot; 1.5k words 
            &middot; 6 minute read
        </p>
        <!-- <p class="post-summary"> <strong>Summary</strong>: </p> -->
    </div>

    
    <div class="collapsed-sidebar">
        <nav id="collapsed-toc" class="table-of-contents">
            <hr class="toc-separator">
            <ul>
                
                <li class="toc-h2">
                    <a href="#section-0">Summary</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-1">Set Up</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-2">Finding features</a>
                </li>
                
                <li class="toc-h3">
                    <a href="#section-3">Positional features</a>
                </li>
                
                <li class="toc-h3">
                    <a href="#section-4">Semantic features</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-5">Length generalization of SAEs</a>
                </li>
                
                <li class="toc-h3">
                    <a href="#section-6">Positional features break</a>
                </li>
                
                <li class="toc-h3">
                    <a href="#section-7">Semantic features break</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-8">Discussion</a>
                </li>
                
                <li class="toc-h2">
                    <a href="#section-9">Acknowledgements</a>
                </li>
                
            </ul>
            <hr class="toc-separator">
        </nav>
    </div>
    

    <div class="post-content">
        <p><em>This is an informal research note. It is the result of a few-day exploration into positional SAE features conducted as part of Neel Nanda’s training phase of the ML Alignment &amp; Theory Scholars Program - Summer 2024 cohort.</em></p>
<h2 id="section-0">Summary</h2>
<p><img alt="" src="assets/1.png"/></p>
<p>Figure 1: <strong>(Dots)</strong> The top 3 PCA components of rows 1 to 127 of gpt2-small’s positional embedding matrix explain 95% of their variance. <strong>(Crosses)</strong> SAEs trained on layer 0 residual stream activations learn many features that together recover this 1 dimensional helical manifold. Colour corresponds to the position on which the feature is most active. Blue corresponds to position 1, red corresponds to position 127. The position 0 row and SAE features are omitted (as they are weird).</p>
<p>We investigate <em>positional SAE features</em> learned by layer 0 residual stream SAEs trained on gpt2-small. In particular, we study the activation <code>blocks.0.hook_resid_pre</code>, which is the sum of the token embeddings and positional embeddings. Importantly gpt2-small uses absolute learned positional embeddings – that is, the positional embeddings are a trainable parameter (learned) and are injected into the residual stream (absolute).</p>
<p>We find that this SAE learns a set of <em>positional features</em>. We investigate some of the properties of these features, finding</p>
<ul>
<li><em>Positional</em> and <em>semantic</em> features are (almost) entirely disjoint at layer 0.  Note that we do not expect this to continue holding in later layers as attention mixes semantic and positional information. In layer 0, we should <em>expect</em> the SAE to disentangle positional and semantic features as there is a natural notion of ground truth positional and semantic features that interact purely additively. </li>
<li>Generically, each positional feature spans a range of positions, except for the first few positions which each get dedicated (and sometimes, several) features.</li>
<li>We can attribute <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream?commentId=5d5eKJJrDkwzQbKgf">degradation</a> of SAE performance beyond the SAE training context length to (lack of) these positional features, and to the absolute nature of positional embeddings used by this model.</li>
</ul>
<h2 id="section-1">Set Up</h2>
<p>We study pretrained gpt2-small SAEs trained on <code>blocks.0.hook_resid_pre</code>. This is particularly clean, as we can generate the entire input distribution to the SAE by summing each of the <code>d_vocab</code> token embeddings with each of the <code>n_ctx</code> positional embeddings, obtaining a tensor</p>
<p><code>all_resid_pres: Float[Tensor, “d_vocab n_ctx d_model”]</code> </p>
<p>By passing this tensor through the SAE, we can grab all of the pre/post activation function feature activations</p>
<p><code>all_feature_acts: Float[Tensor, “d_vocab n_ctx d_sae”]</code></p>
<p>In this post, <code>d_model = 768</code> and <code>d_sae = 24576</code>. Importantly the SAE we study in this post has <code>context_size=128</code>. The SAE context size corresponds is the maximal length of input sequence used to generate activations for training of the SAE.</p>
<h2 id="section-2">Finding features</h2>
<p>The activation space of study can be thought of as the direct sum of the token embedding space and the positional embedding space. As such, we hypothesize that semantic and positional features learned by the SAE should be distinct.</p>
<p>That is, we hypothesize that the feature activations for some feature i can be written in the form </p>
<p>$f_i(x_{tok}+x_{pos})=g_i(x_{tok})+h_i(x_{pos})$</p>
<p>where for each $i$, either $g_i=0$ or $h_i=0$ identically for all inputs in their domain and $x$ is a <code>d_model</code> dimensional vector.</p>
<p>To investigate this we hold <code>tok</code> or <code>pos</code> fixed in <code>all_feature_acts</code> and vary the other input. We first restrict to <code>pos &lt; sae.cfg.context_size</code>. </p>
<h3 id="section-3">Positional features</h3>
<p>We first replicate Figure 1f of <a href="https://arxiv.org/pdf/2401.12181">Gurnee et al. (2024)</a>, which finds instances of sinusoidal positional <em>neurons</em> in MLP layers.</p>
<p><img alt="" src="assets/2.png"/></p>
<p>To do so, we assign each feature a <em>positional score</em>. We first compute the mean activation of each feature at each position by averaging over all possible input tokens. The position score is the max value of this over all positions, i.e.</p>
<p>$p_i=\max_{pos}\{\text{mean}_{tok}{f_i(tok,pos)}\}$</p>
<p>where $f_i(tok,pos)$ is the feature activation for feature i for the given input. </p>
<p>We find positional scores drop off rapidly. There seem to only be ~50 positional features (of 24k total features) in this SAE. </p>
<p><img alt="" src="assets/3.png"/></p>
<p>Inspecting the features, we find</p>
<ol>
<li>Many positional features, each with small standard deviation over input tokens (shown in lower opacity below, indicating they really are true positional features). These span the entire context length.</li>
<li>The pre-ReLU feature activations are close to sinusoidal, but only rise above zero (so survive the ReLU) once in the context length of the SAE, so correspond to a <em>localised range</em> of token positions.</li>
<li>Early positions fire more strongly than later positional features, and there seem to be many more of them. Our metric gives them higher positional scores than other positional features.</li>
<li>All positions get some positional features, but later positions are forced to share their features across several positions, while early positions get proportionally more dedicated features. In Figure 1, we see that the the ground truth positional embeddings are more spread out for early positions, so this is to be expected.</li>
</ol>
<p><img alt="" src="assets/4.png"/></p>
<p>We directly compare the learned feature directions to the W_pos matrix in Figure 1, by plotting the position of the peak of each positional feature bump.</p>
<h3 id="section-4">Semantic features</h3>
<p>Semantic features are relatively well studied, so we only briefly study them here. In particular, we can find the most important features for a given token by assigning <em>token scores</em> for each feature for a particular token by instead taking the mean over all <em>positions</em>. Below we plot the top-k features for a particular token. We generally find that for a fixed token there are some strong/weakly activating token dependent features (horizontal lines), and a set of lower magnitude position dependent features (bumps) that fire conditional on the sequence position.</p>
<p><img alt="" src="assets/5.png"/></p>
<h2 id="section-5">Length generalization of SAEs</h2>
<p>It has <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream?commentId=5d5eKJJrDkwzQbKgf">been</a> <a href="https://www.lesswrong.com/posts/8QRH8wKcnKGhpAu2o/examining-language-model-performance-with-reconstructed#How_does_context_length_affect_SAE_performance_on_randomly_sampled_data_">observed</a> that SAEs sometimes break out of the training context length of activations they were trained on. In particular, the reconstruction error blows up. Note that the reconstruction error is significantly worse than what one would obtain by randomly reconstructing a vector of similar norm.</p>
<p><img alt="" src="assets/6.png"/></p>
<p>Why does this happen? We provide several lines of evidence.</p>
<h3 id="section-6">Positional features break</h3>
<p>We now extend the plot of feature activations of positional features to positions beyond the context length. Things get pretty weird….</p>
<p><img alt="" src="assets/7.png"/></p>
<p>Positional features are trained on short contexts, and overfit to them. This plot suggests that at late positions the SAE “thinks” that it is at significantly earlier positions, as each feature in the training distribution only corresponds to a highly local region.</p>
<h3 id="section-7">Semantic features break</h3>
<p>Semantic features <em>also</em> break. The features which fire for given tokens have stable activations until the end of the SAE context length, but then become noisier.</p>
<p><img alt="" src="assets/8.png"/></p>
<p>Additionally, features that did not fire for small positions also suddenly start firing as the position is increased. <em>The SAE thinks tokens are present which are not</em><strong>.</strong> We can easily validate this in <a href="https://www.neuronpedia.org/">neuronpedia</a>. Here’s a feature that “should” fire on arrows (e.g. “-&gt;”). However, it It also fires on the word “leep” late in the context, making the SAE "think" there are arrows present.</p>
<p><img alt="" src="assets/9.png"/></p>
<h2 id="section-8">Discussion</h2>
<p><strong>Why does the SAE not learn one feature for each position?</strong> As we can see in Figure 1 and Figure 3, The SAE learns fewer positional features than positions. Features mostly correspond to a range of positions. This shouldn’t be surprising – we see in Figure 1 that the cosine sim of adjacent positions is very high. The SAE sparsity penalty penalises learning highly similar features. We suspect training SAEs to recover positional encodings might be a good toy set up for studying <em>feature splitting</em>, and predict the number of positional features should behave predictably under scaling SAE width.</p>
<p><strong>Thoughts on SAE feature geometry.</strong> While it is possible to assign highly interpretable and accurate explanations to each positional SAE feature, such explanations lose sight of the rich feature geometry of the positional embedding. <a href="https://www.lesswrong.com/posts/MFBTjb2qf3ziWmzz6/sae-feature-geometry-is-outside-the-superposition-hypothesis#The_placement_of_each_feature_vector_in_the_activation_space_matters">The placement of each feature vector in the activation space matters</a>, and communicating the large, inscrutable <code>W_dec</code> matrix of the SAE (as we do in Figure 1) does not constitute an explanation. The SAE features present in circular temporal features found by <a href="https://arxiv.org/pdf/2405.14860">Engels et al.</a> suffer a similar problem.</p>
<p><strong>What’s up with length generalization?</strong> The SAE encoder attempts to disentangle features in superposition, assigning a coefficient to each feature despite there existing many other features "nearby". </p>
<p>However, it has never seen <code>W_pos[128:]</code>. On top of not being able to reconstruct these positional features at all, it <em>also</em> can’t minimize interference of <code>W_pos[128:]</code> with positional features for positions lower than the context length, or the token-like semantic features which it has already learned. What we are observing in the large reconstruction errors are these two separate types of error. This is pretty fundamentally broken. It’s possible that finetuning the SAE on a small amount of longer context activations would be sufficient for fixing this.</p>
<p>That said, we note that the length generalization results may not be all that important, as modern LLMs do not use absolute, learned positional embeddings. SAEs trained on Pythia, which uses <a href="https://arxiv.org/abs/2104.09864">rotary embeddings</a>, empirically <a href="https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream#5d5eKJJrDkwzQbKgf">don’t seem to suffer the same problem</a>. Indeed, one of the primary motivations of rotary embeddings in the first place was to improve LLM length generalization.</p>
<h2 id="section-9">Acknowledgements</h2>
<p><em>Bilal and Yeu-Tong pair programmed on most of this work. Bilal wrote up this post. Thanks to Andy Arditi, Arthur Conmy and Stefan Heimersheim for helpful feedback. Thanks to Joseph Bloom for training this SAE.</em></p>
    </div>
    <div class="footer">
        
        
        
        <p>There may be comments and footnotes on the version of this post that was published on:</p>
        <ul>
            
            <li><a href="https://www.lesswrong.com/posts/ctGeJGHg9pbc8memF/understanding-positional-features-in-layer-0-saes">www.lesswrong.com</a></li>
            
        </ul>
        </p>
        
    </div>
</article>

            </div>
        </div>
        <!--<div class="column right">
            <div class="sticky-div">
                
            </div>
        </div>-->
    </div>
    <footer></footer>
</body>
</html>